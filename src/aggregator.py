#!/usr/bin/env python3
"""
ç®€å•çš„ RSS èšåˆå™¨ï¼š
- è¯»å– `feeds.yaml` ä¸­çš„ feed
- æŠ“å–è¿‘ 24 å°æ—¶å†…çš„æ–‡ç« 
- æå–æ¯ç¯‡æ–‡ç« çš„ç¬¬ä¸€æ®µæˆ– meta æè¿°ä½œä¸ºè¦ç‚¹
- è¾“å‡º `out/digest-YYYYMMDD.md`

ä½¿ç”¨æ–¹æ³•ï¼š
  python src/aggregator.py
"""
import os
import sys
import yaml
import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timezone, timedelta
from pathlib import Path

OUT_DIR = Path("out")
OUT_DIR.mkdir(exist_ok=True)


def load_feeds(path="feeds.yaml"):
    with open(path, "r", encoding="utf-8") as f:
        data = yaml.safe_load(f)
    return data.get("feeds", [])


def entry_published_dt(entry):
    # å°è¯•ä½¿ç”¨ published_parsed/updated_parsed
    for key in ("published_parsed", "updated_parsed"):
        t = entry.get(key)
        if t:
            return datetime.fromtimestamp(t, tz=timezone.utc) if isinstance(t, (int, float)) else None
    return None


def is_ai_related(text):
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä¸ AI ç›¸å…³"""
    if not text:
        return True  # æ— æ–‡æœ¬æ—¶ä¸è¿‡æ»¤
    text_lower = text.lower()
    ai_keywords = [
        "ai", "artificial intelligence", "äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ",
        "deep learning", "æ·±åº¦å­¦ä¹ ", "neural", "ç¥ç»ç½‘ç»œ",
        "llm", "å¤§æ¨¡å‹", "gpt", "claude", "gemini",
        "transformer", "æ³¨æ„åŠ›æœºåˆ¶", "ç”Ÿæˆå¼", "generative",
        "ç®—æ³•", "algorithm", "æ¨¡å‹", "model", "è®­ç»ƒ",
        "æ¨ç†", "inference", "å¾®è°ƒ", "fine-tune", "prompt",
        "embedding", "å‘é‡", "nlp", "è‡ªç„¶è¯­è¨€", "vision", "è§†è§‰"
    ]
    return any(keyword in text_lower for keyword in ai_keywords)


def fetch_summary(url, fallback=""):
    try:
        resp = requests.get(url, timeout=8, headers={"User-Agent": "Mozilla/5.0"})
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")
        # ä¼˜å…ˆå–ç¬¬ä¸€æ®µè¾ƒé•¿çš„ <p>
        for p in soup.find_all("p"):
            text = p.get_text(strip=True)
            if len(text) >= 40:
                return text
        # å°è¯• meta description
        md = soup.find("meta", attrs={"name": "description"})
        if md and md.get("content"):
            return md.get("content").strip()
    except Exception:
        pass
    return fallback


def build_digest(items):
    today = datetime.now().strftime("%Y%m%d")
    path = OUT_DIR / f"digest-{today}.md"
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_count = len(items)
    source_counts = {}
    for item in items:
        source = item['source']
        source_counts[source] = source_counts.get(source, 0) + 1
    unique_sources = len(source_counts)
    
    with open(path, "w", encoding="utf-8") as f:
        f.write(f"# AI æ¯æ—¥å¿«è®¯ â€” {datetime.now().strftime('%Y-%m-%d')}\n\n")
        
        # æ·»åŠ æ€»ç»“æ€§ç»Ÿè®¡
        f.write("## ğŸ“Š ä»Šæ—¥æ¦‚è§ˆ\n\n")
        f.write(f"- **ğŸ“° æ€»è®¡æ–‡ç« æ•°**ï¼š{total_count} ç¯‡\n")
        f.write(f"- **ğŸ”— ä¿¡æ¯æºæ•°**ï¼š{unique_sources} ä¸ª\n")
        f.write("\n### å„æºæ–‡ç« æ•°\n\n")
        for source in sorted(source_counts.keys()):
            count = source_counts[source]
            f.write(f"- {source}ï¼š{count} ç¯‡\n")
        
        f.write("\n---\n\n")
        f.write("## ğŸ“° è¯¦ç»†å†…å®¹\n\n")
        f.write("æœ¬æ‘˜è¦ä¸ºè¿‘ 24 å°æ—¶å†…èšåˆå†…å®¹ï¼ŒæŒ‰æ¥æºæ’åºã€‚\n\n")
        
        for it in items:
            f.write(f"- **{it['source']}**: [{it['title']}]({it['link']})\n")
            if it.get("summary"):
                f.write(f"  \n  è¦ç‚¹: {it['summary']}\n\n")
            else:
                f.write("\n")
    print("Wrote:", path)
    return path


def main():
    feeds = load_feeds()
    cutoff = datetime.now(timezone.utc) - timedelta(hours=24)
    all_items = []
    seen = set()
    for feed in feeds:
        name = feed.get("name")
        url = feed.get("url")
        if not url:
            continue
        try:
            d = feedparser.parse(url)
        except Exception:
            continue
        for entry in d.entries:
            link = entry.get("link")
            if not link:
                continue
            if link in seen:
                continue
            pubdt = entry_published_dt(entry)
            # è‹¥æ— æ—¶é—´ä¿¡æ¯ï¼Œä¿å®ˆåœ°åŒ…å«ï¼ˆå› ä¸ºå¯èƒ½æ˜¯åˆšå‘å¸ƒï¼‰
            if pubdt and pubdt < cutoff:
                continue
            seen.add(link)
            title = entry.get("title", "(no title)")
            fallback = entry.get("summary", "")
            
            # AI å…³é”®å­—è¿‡æ»¤ï¼šæ ‡é¢˜æˆ–æ‘˜è¦å¿…é¡»åŒ…å« AI ç›¸å…³è¯æ±‡
            if not (is_ai_related(title) or is_ai_related(fallback)):
                continue
            
            summary = fetch_summary(link, fallback=fallback)
            # ç®€çŸ­åŒ– summary ä¸ºä¸€ä¸¤å¥
            if summary and len(summary) > 300:
                summary = summary[:300].rsplit('.', 1)[0] + '.'

            all_items.append({
                "source": name or d.feed.get("title", "unknown"),
                "title": title,
                "link": link,
                "summary": summary,
            })

    if not all_items:
        print("No new items in last 24 hours.")
        return
    # æŒ‰æ¥æºæ’åº
    all_items.sort(key=lambda x: x.get("source", ""))
    build_digest(all_items)


if __name__ == "__main__":
    main()
